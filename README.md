# ä½¿ç”¨Piperæ•°æ®å¾®è°ƒGR00T N1.6
## 1. é…ç½®ç¯å¢ƒ
é…ç½®hdf5è½¬lerobot dataset v2.1çš„ç¯å¢ƒ
```bash
git clone https://github.com/12e21/piper_gr00t.git
conda create -n hdf2lerobot python=3.10
conda activate hdf2lerobot
conda install ffmpeg=7 -c conda-forge
pip install uv
uv pip install lerobot==0.4.2 h5py opencv-python ipdb typer tqdm numpy datatrove
```

## 2. ä½¿ç”¨è„šæœ¬è½¬æ¢hdf5æ–‡ä»¶

```bash
python hdf2lerobotv21.py --all \ # è½¬æ¢ç›®å½•ä¸‹æ‰€æœ‰hdf5æ–‡ä»¶
  --repo-id "your/repo" \ # ä»“åº“id
  --hdf5-root "./data" \ # å­˜æ”¾hdf5æ–‡ä»¶çš„ç›®å½•
  --push # ä¸Šä¼ åˆ°huggingface
```
è„šæœ¬æ”¯æŒä»¥ä¸‹æ•°æ®æ ¼å¼ï¼š

- **åŠ¨ä½œ**: 14 ç»´å…³èŠ‚ä½ç½®ï¼ˆåŒè‡‚å„ 7 ä¸ªå…³èŠ‚ï¼‰
- **è§‚æµ‹**: 14 ç»´å…³èŠ‚ä½ç½®çŠ¶æ€
- **å›¾åƒ**: 3 ä¸ªè§†è§’ï¼ˆå·¦æ‰‹è…•ã€ä¸­é—´ã€å³æ‰‹è…•ï¼‰ï¼Œ480x640 åˆ†è¾¨ç‡


## 3. é…ç½®GR00T N1.6ç¯å¢ƒ
```bash
git clone --recurse-submodules https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
git submodule update --init --recursive

conda create -n gr00t16 python=3.10 -y && conda activate gr00t16
pip install uv

uv pip install -e .
uv pip install jsonlines lerobot
conda install ffmpeg=7 -c conda-forge

hf download nvidia/GR00T-N1.6-3B
```

## 4. è®¾ç½®æ•°æ®é›†å‚æ•°
1. å°†`config_for_gr00tn16`ç›®å½•å¤åˆ¶åˆ°GR00Tçš„æ ¹ç›®å½•ä¸‹
2. å°†`config_for_gr00tn16/modality.json`å­˜æ”¾åœ¨lerobotæ•°æ®é›†çš„`meta`ç›®å½•ä¸‹
  - å¦‚æœéœ€è¦ä¿®æ”¹Actionçš„ç»å¯¹å’Œç›¸å¯¹è¡¨ç¤ºï¼Œå¯ä»¥åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ä¿®æ”¹ï¼Œé»˜è®¤é‡‡ç”¨armä¸ºç›¸å¯¹ï¼Œgripperä¸ºç»å¯¹
3. å¯åŠ¨å¾®è°ƒè„šæœ¬ï¼ˆæ•°æ®é›†å­˜æ”¾ä½ç½®ï¼ŒGPUæ•°é‡ï¼Œå¾®è°ƒæ­¥æ•°ç­‰è®¾ç½®è¯·åœ¨è„šæœ¬ä¸­ä¿®æ”¹ï¼‰
```bash
bash config_for_gr00tn16/finetune_bi_piper.sh
```

## æµ‹è¯•
1. hdf5 è½¬æ¢ Lerobot Datasetè½¬æ¢æˆåŠŸï¼Œé€Ÿåº¦ä¸æ˜¯å¾ˆå¿«
2. ä½¿ç”¨ä¸¤ä¸ªepisodeè½¬æ¢å¾—åˆ°çš„æ•°æ®é›†å¾®è°ƒGR00T N1.6ï¼Œä½¿ç”¨ä¸¤å¼ A100 80Gï¼ŒLossæ­£å¸¸ä¸‹é™

# HDF5 Toolsä½¿ç”¨

## read_hdf5.py - HDF5 æ–‡ä»¶æŸ¥çœ‹å·¥å…·

å¿«é€ŸæŸ¥çœ‹ HDF5 æ–‡ä»¶çš„æ•°æ®ç»“æ„ã€å±æ€§å’Œæ•°æ®å†…å®¹ã€‚

### åŸºæœ¬ç”¨æ³•
```bash
# åŸºæœ¬æŸ¥çœ‹æ–‡ä»¶ç»“æ„
python hdf5_tools/read_hdf5.py data.h5

# æ˜¾ç¤ºå±æ€§å’Œæ•°æ®é¢„è§ˆ
python hdf5_tools/read_hdf5.py data.h5 --attrs --preview

# é™åˆ¶æ˜¾ç¤ºå±‚çº§ï¼ˆé€‚ç”¨äºæ·±å±‚åµŒå¥—ç»“æ„ï¼‰
python hdf5_tools/read_hdf5.py data.h5 --max-level 2
```

### äº¤äº’å¼æ¨¡å¼
```bash
python hdf5_tools/read_hdf5.py data.h5 --interactive
```

äº¤äº’æ¨¡å¼å¯ç”¨å‘½ä»¤ï¼š
- `help` - æ˜¾ç¤ºå¸®åŠ©
- `cd <name>` - è¿›å…¥ç»„ï¼ˆä½¿ç”¨ `..` è¿”å›ä¸Šçº§ï¼‰
- `ls` - åˆ—å‡ºå½“å‰ç»„å†…å®¹
- `info <name>` - æ˜¾ç¤ºæ•°æ®é›†è¯¦ç»†ä¿¡æ¯
- `preview <name>` - é¢„è§ˆæ•°æ®é›†æ•°æ®
- `pwd` - æ˜¾ç¤ºå½“å‰ä½ç½®
- `exit` æˆ– `quit` - é€€å‡º

---

## split_hdf5.py - HDF5 æ–‡ä»¶æ‹†åˆ†å·¥å…·

å°†åŒ…å«å¤šä¸ª group çš„ HDF5 æ–‡ä»¶æ‹†åˆ†æˆå¤šä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼Œæ¯ä¸ª group ä¿å­˜ä¸ºç‹¬ç«‹çš„ HDF5 æ–‡ä»¶ã€‚

### åˆ—å‡ºæ–‡ä»¶ä¸­çš„ groups
```bash
python hdf5_tools/split_hdf5.py list-hdf5-groups --input data.h5
```

### æ‹†åˆ†æ–‡ä»¶
```bash
# æ‹†åˆ†æ‰€æœ‰ groups
python hdf5_tools/split_hdf5.py split-hdf5-file \
  --input data.h5 \
  --output ./split_output

# æ‹†åˆ†æŒ‡å®šçš„ groups
python hdf5_tools/split_hdf5.py split-hdf5-file \
  --input data.h5 \
  --output ./split_output \
  --groups episode_0 episode_1

# æ·»åŠ æ–‡ä»¶åå‰ç¼€
python hdf5_tools/split_hdf5.py split-hdf5-file \
  --input data.h5 \
  --output ./split_output \
  --prefix "piper_"

# è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
python hdf5_tools/split_hdf5.py split-hdf5-file \
  --input data.h5 \
  --output ./split_output \
  --overwrite
```

### çŸ­å‚æ•°å½¢å¼
```bash
python hdf5_tools/split_hdf5.py split-hdf5-file -i data.h5 -o ./split_output
```

---

## repack_hdf5.py - HDF5 æ–‡ä»¶é‡æ–°æ‰“åŒ…å·¥å…·

å°†ç›®å½•ä¸­çš„å¤šä¸ª HDF5 æ–‡ä»¶é‡æ–°åˆ’åˆ†æˆåŒ…å«æŒ‡å®šæ•°é‡ episodes çš„ HDF5 æ–‡ä»¶ã€‚

ä¾‹å¦‚ï¼šè¾“å…¥ç›®å½•åŒ…å« file1.hdf5 (10 episodes) å’Œ file2.hdf5 (15 episodes)ï¼ŒæŒ‡å®šæ¯æ–‡ä»¶ 5 ä¸ª episodesï¼Œå°†ç”Ÿæˆ 5 ä¸ªè¾“å‡ºæ–‡ä»¶ï¼Œæ¯ä¸ªåŒ…å« 5 ä¸ª episodesã€‚

### åˆ†æç›®å½•ä¸­çš„ episodes åˆ†å¸ƒ
```bash
python convert_parallel/repack_hdf5.py analyze \
  --input ./data \
  --episodes-per-file 50

# æŒ‡å®šæ–‡ä»¶åŒ¹é…æ¨¡å¼
python convert_parallel/repack_hdf5.py analyze \
  --input ./data \
  --pattern "*.hdf5" \
  --episodes-per-file 100
```

### é¢„è§ˆé‡æ–°æ‰“åŒ…ç»“æœ
```bash
python convert_parallel/repack_hdf5.py repack \
  --input ./data \
  --output ./repacked \
  --episodes-per-file 50 \
  --dry-run
```

### æ‰§è¡Œé‡æ–°æ‰“åŒ…
```bash
# åŸºæœ¬ç”¨æ³•
python convert_parallel/repack_hdf5.py repack \
  --input ./data \
  --output ./repacked \
  --episodes-per-file 50

# è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶åå‰ç¼€
python convert_parallel/repack_hdf5.py repack \
  --input ./data \
  --output ./repacked \
  --episodes-per-file 100 \
  --prefix "shard_"

# è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
python convert_parallel/repack_hdf5.py repack \
  --input ./data \
  --output ./repacked \
  --episodes-per-file 50 \
  --overwrite

# çŸ­å‚æ•°å½¢å¼
python convert_parallel/repack_hdf5.py repack \
  -i ./data \
  -o ./repacked \
  -e 50
```

---

## convert_hdf5_shards.py - HDF5 å¹¶è¡Œè½¬æ¢å·¥å…·

ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå°† HDF5 æ–‡ä»¶è½¬æ¢ä¸º LeRobot Datasetï¼Œæ¯ä¸ª worker å¤„ç†éƒ¨åˆ†æ–‡ä»¶å¹¶ç”Ÿæˆç‹¬ç«‹çš„ shardï¼Œæœ€åå¯èšåˆä¸ºå®Œæ•´æ•°æ®é›†ã€‚

### å®‰è£…ä¾èµ–
```bash
pip install datatrove lerobot
```

### æœ¬åœ°å¹¶è¡Œè½¬æ¢ï¼ˆLocalPipelineExecutorï¼‰
```bash
# åŸºæœ¬ç”¨æ³•ï¼šä½¿ç”¨ 4 ä¸ª worker å¹¶è¡Œå¤„ç†æ‰€æœ‰ HDF5 æ–‡ä»¶
python convert_parallel/convert_hdf5_shards.py \
  --hdf5-root ./data \
  --all \
  --repo-id "12e21/bi_piper_parallel" \
  --workers 4

# æŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶
python convert_parallel/convert_hdf5_shards.py \
  --hdf5-root ./data \
  --hdf5-files file1.hdf5 file2.hdf5 file3.hdf5 \
  --repo-id "12e21/bi_piper_parallel" \
  --workers 8

# è‡ªå®šä¹‰å‚æ•°
python convert_parallel/convert_hdf5_shards.py \
  --hdf5-root ./data \
  --all \
  --repo-id "12e21/bi_piper_parallel" \
  --robot-type bi_piper \
  --fps 30 \
  --workers 8 \
  --job-name "convert_bi_piper"
```

### SLURM é›†ç¾¤å¹¶è¡Œè½¬æ¢
```bash
python convert_parallel/convert_hdf5_shards.py \
  --hdf5-root ./data \
  --all \
  --repo-id "12e21/bi_piper_parallel" \
  --workers 100 \
  --slurm 1 \
  --partition cpu \
  --cpus-per-task 8 \
  --mem-per-cpu 4000M \
  --job-name "convert_bi_piper"
```

### è¾“å‡ºç»“æ„
è½¬æ¢åä¼šç”Ÿæˆå¤šä¸ª shardsï¼š
```
12e21/bi_piper_parallel_world_4_rank_0  # Worker 0 å¤„ç†çš„æ–‡ä»¶
12e21/bi_piper_parallel_world_4_rank_1  # Worker 1 å¤„ç†çš„æ–‡ä»¶
12e21/bi_piper_parallel_world_4_rank_2  # Worker 2 å¤„ç†çš„æ–‡ä»¶
12e21/bi_piper_parallel_world_4_rank_3  # Worker 3 å¤„ç†çš„æ–‡ä»¶
```

### èšåˆ Shards
```bash
# å°†æ‰€æœ‰ shards èšåˆä¸ºä¸€ä¸ªå®Œæ•´æ•°æ®é›†
python convert_parallel/aggregate_hdf5_shards.py \
  --repo-id "12e21/bi_piper_parallel" \
  --num-shards 4
```

### datatrove æ—¥å¿—ç³»ç»Ÿ

`convert_hdf5_shards.py` ä½¿ç”¨ datatrove æ¡†æ¶è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œä¼šè‡ªåŠ¨åœ¨ `./logs/` ç›®å½•ä¸‹åˆ›å»ºæ—¥å¿—æ–‡ä»¶ç”¨äº**æ–­ç‚¹ç»­ä¼ **å’Œ**ä»»åŠ¡è¿½è¸ª**ï¼š

```
logs/convert_hdf5/
â”œâ”€â”€ executor.json          # æ‰§è¡Œå™¨é…ç½®å’ŒçŠ¶æ€ï¼ˆworkers, tasks, pipelineç­‰ï¼‰
â”œâ”€â”€ completions/           # ä»»åŠ¡å®Œæˆæ ‡è®°ï¼ˆ00000, 00001, ...ï¼‰
â”œâ”€â”€ logs/                  # æ¯ä¸ª worker çš„è¯¦ç»†è¿è¡Œæ—¥å¿—
â””â”€â”€ stats/                 # å¤„ç†é€Ÿåº¦ã€æ•°æ®é‡ç­‰ç»Ÿè®¡ä¿¡æ¯
```

#### é‡å¤è¿è¡Œçš„é—®é¢˜

å¦‚æœçœ‹åˆ°ä»¥ä¸‹æç¤ºï¼š
```
Not doing anything as all X tasks have already been completed.
```

è¯´æ˜ datatrove æ£€æµ‹åˆ°ä¹‹å‰çš„ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆã€‚è§£å†³æ–¹æ³•ï¼š

**æ–¹æ³• 1ï¼šæ¸…ç©ºæ—¥å¿—ç›®å½•**
```bash
rm -rf ./logs/convert_hdf5/
```

**æ–¹æ³• 2ï¼šä½¿ç”¨ä¸åŒçš„ job åç§°**
```bash
python convert_parallel/convert_hdf5_shards.py \
  --hdf5-root ./data \
  --all \
  --repo-id "12e21/bi_piper_parallel" \
  --workers 4 \
  --job-name "convert_run2"  # æ–°æ—¥å¿—ç›®å½•ï¼šlogs/convert_run2/
```

#### æ³¨æ„äº‹é¡¹
- âœ… ä¼˜ç‚¹ï¼šæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œä»»åŠ¡ä¸­æ–­åå¯ç»§ç»­
- âš ï¸ æ³¨æ„ï¼šæ›´æ”¹ `--workers` æ•°é‡æ—¶ï¼Œéœ€è¦æ¸…ç©ºæ—¥å¿—æˆ–ä½¿ç”¨æ–°çš„ `--job-name`
- ğŸ“ æ—¥å¿—ä½ç½®ï¼šé»˜è®¤åœ¨ `./logs/<job-name>/`

---

## aggregate_hdf5_shards.py - Shards èšåˆå·¥å…·

å°† `convert_hdf5_shards.py` ç”Ÿæˆçš„å¤šä¸ª shards èšåˆä¸ºä¸€ä¸ªå®Œæ•´çš„ LeRobot Datasetã€‚

### åŸºæœ¬ç”¨æ³•
```bash
python convert_parallel/aggregate_hdf5_shards.py \
  --repo-id "12e21/bi_piper_parallel" \
  --num-shards 4
```

### æŒ‡å®šè¾“å‡ºæ•°æ®é›†åç§°
```bash
python convert_parallel/aggregate_hdf5_shards.py \
  --repo-id "12e21/bi_piper_parallel" \
  --num-shards 4 \
  --output-repo-id "12e21/bi_piper_final"
```

### SLURM é›†ç¾¤è¿è¡Œ
```bash
python convert_parallel/aggregate_hdf5_shards.py \
  --repo-id "12e21/bi_piper_parallel" \
  --num-shards 100 \
  --slurm 1 \
  --partition cpu \
  --job-name "aggregate_bi_piper"
```

### å‚æ•°è¯´æ˜
| å‚æ•° | è¯´æ˜ |
|------|------|
| `--repo-id` | åŸºç¡€ repo IDï¼ˆä¸å« _world_X_rank_Y åç¼€ï¼‰|
| `--num-shards` | Shard æ•°é‡ï¼ˆåº”ç­‰äº convert æ—¶çš„ --workersï¼‰|
| `--output-repo-id` | è¾“å‡ºæ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ --repo-idï¼‰|
| `--slurm` | ä½¿ç”¨ SLURMï¼ˆ1=å¯ç”¨ï¼Œ0=æœ¬åœ°ï¼‰|
| `--workers` | Worker æ•°é‡ï¼ˆèšåˆåº”è®¾ä¸º 1ï¼‰|
| `--partition` | SLURM åˆ†åŒºåç§° |
| `--cpus-per-task` | æ¯ä¸ª task çš„ CPU æ•°é‡ |
| `--mem-per-cpu` | æ¯ä¸ª CPU çš„å†…å­˜ |
| `--logs-dir` | æ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤ï¼š./logsï¼‰|
| `--job-name` | ä»»åŠ¡åç§° |

### å®Œæ•´å·¥ä½œæµç¤ºä¾‹
```bash
# Step 1: å¹¶è¡Œè½¬æ¢ï¼ˆ100 ä¸ª workersï¼‰
python convert_parallel/convert_hdf5_shards.py \
  --hdf5-root ./data \
  --all \
  --repo-id "12e21/bi_piper_parallel" \
  --workers 100

# Step 2: èšåˆæ‰€æœ‰ shards
python convert_parallel/aggregate_hdf5_shards.py \
  --repo-id "12e21/bi_piper_parallel" \
  --num-shards 100

# ç°åœ¨å¯ä»¥ä½¿ç”¨å®Œæ•´æ•°æ®é›† "12e21/bi_piper_parallel"
```

---

### convert_hdf5_shards.py å‚æ•°è¯´æ˜
| å‚æ•° | è¯´æ˜ |
|------|------|
| `--hdf5-root` | HDF5 æ–‡ä»¶æ ¹ç›®å½• |
| `--all` | å¤„ç†ç›®å½•ä¸­æ‰€æœ‰ HDF5 æ–‡ä»¶ |
| `--hdf5-files` | æŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶ï¼ˆå¯å¤šä¸ªï¼‰|
| `--repo-id` | HuggingFace ä»“åº“ ID |
| `--robot-type` | æœºå™¨äººç±»å‹ï¼ˆé»˜è®¤ï¼šbi_piperï¼‰|
| `--fps` | è§†é¢‘å¸§ç‡ï¼ˆé»˜è®¤ï¼š30ï¼‰|
| `--workers` | å¹¶è¡Œ worker æ•°é‡ |
| `--slurm` | ä½¿ç”¨ SLURMï¼ˆ1=å¯ç”¨ï¼Œ0=æœ¬åœ°ï¼‰|
| `--partition` | SLURM åˆ†åŒºåç§° |
| `--cpus-per-task` | æ¯ä¸ª task çš„ CPU æ•°é‡ |
| `--mem-per-cpu` | æ¯ä¸ª CPU çš„å†…å­˜ |
| `--logs-dir` | æ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤ï¼š./logsï¼‰|
| `--job-name` | ä»»åŠ¡åç§° |

---

## lerobot_v30_to_v21.py - LeRobot æ•°æ®é›†ç‰ˆæœ¬è½¬æ¢å·¥å…·

å°† LeRobot æ•°æ®é›†ä» codebase ç‰ˆæœ¬ v3.0 è½¬æ¢å› v2.1 æ ¼å¼ã€‚

### ç¯å¢ƒè¦æ±‚
- LeRobot ç‰ˆæœ¬è‡³å°‘éœ€è¦åœ¨ commit `f55c6e89f` ä¹‹å
- å·²åœ¨ LeRobot ç‰ˆæœ¬ 0.4.0 (commit: f25ac02) ä¸Šæµ‹è¯•é€šè¿‡

### è½¬æ¢è¯´æ˜
è„šæœ¬ä¼šå°†æœ¬åœ°çš„ v3.0 æ•°æ®é›†è½¬æ¢ä¸º v2.1 æ ¼å¼ï¼š
- åŸ v3.0 è·¯å¾„ä¼šè¢« v2.1 è·¯å¾„è¦†ç›–
- åŸå§‹ v3.0 æ•°æ®é›†ä¼šå¤‡ä»½åˆ°å¸¦ `_v30` åç¼€çš„æ–‡ä»¶å¤¹ä¸­

### åŸºæœ¬ç”¨æ³•
```bash
# è½¬æ¢ HuggingFace ä¸Šçš„æ•°æ®é›†
python convert_parallel/lerobot_v30_to_v21.py \
  --repo-id "lerobot/pusht"

# è½¬æ¢æœ¬åœ°æ•°æ®é›†
python convert_parallel/lerobot_v30_to_v21.py \
  --repo-id "lerobot/pusht" \
  --root "/path/to/datasets"

# å¼ºåˆ¶é‡æ–°ä¸‹è½½å¹¶è½¬æ¢
python convert_parallel/lerobot_v30_to_v21.py \
  --repo-id "lerobot/pusht" \
  --force-conversion
```

### å‚æ•°è¯´æ˜
| å‚æ•° | è¯´æ˜ |
|------|------|
| `--repo-id` | HuggingFace ä»“åº“æ ‡è¯†ç¬¦ï¼ˆå¿…éœ€ï¼Œä¾‹å¦‚ `lerobot/pusht`ï¼‰|
| `--root` | æœ¬åœ°ç›®å½•ï¼Œç”¨äºå­˜å‚¨æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰|
| `--force-conversion` | å¿½ç•¥ç°æœ‰æœ¬åœ°å¿«ç…§ï¼Œä» Hub é‡æ–°ä¸‹è½½ï¼ˆæ ‡å¿—ä½ï¼‰|
